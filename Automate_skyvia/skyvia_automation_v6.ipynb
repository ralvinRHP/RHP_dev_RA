{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/mnt/c/Users/ralvin/OneDrive - Reliant Health Partners/Documents/RHP_dev_RA/Automate_skyvia')\n",
    "import pandas as pd\n",
    "from simple_salesforce import Salesforce, SalesforceLogin\n",
    "import pyodbc, sys, time \n",
    "import requests\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from helper_functions_v3 import create_df, updated_data_pull, upsert, delete_record, insert_records, salesforce_connection, read_sftp_data, read_sftp_directory, data_pull\n",
    "import configparser\n",
    "import json\n",
    "import paramiko\n",
    "import io\n",
    "\n",
    "pd.set_option('display.max_rows', None) \n",
    "pd.set_option('display.max_columns', None) \n",
    "\n",
    "\n",
    "##### retrieve source data\n",
    "config = configparser.ConfigParser()\n",
    "config.read('config_v2.ini')\n",
    "sftp_config = config['sftp']\n",
    "mapping_config = config['mappings']\n",
    "host = sftp_config.get('host')\n",
    "port = sftp_config.getint('port', fallback=22) \n",
    "username = sftp_config.get('user')\n",
    "password = sftp_config.get('password')\n",
    "remote_dir = sftp_config.get('remote_path')\n",
    "json_path = mapping_config.get('json_file_path')\n",
    "\n",
    "\n",
    "\n",
    "Source_data = read_sftp_directory(remote_dir, host, port, username, password) ### list of all source dataframes\n",
    "for i in Source_data:\n",
    "    print(i)\n",
    "    print(f'Source data shape: {Source_data[i].shape}')\n",
    "\n",
    "## sf connection\n",
    "sf = salesforce_connection(sandbox=True)\n",
    "sf_prod = salesforce_connection(sandbox=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_data(json_path, Source_data, sf):\n",
    "    ## get data mappings \n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    mappings = {k:v for k,v in data.items()}\n",
    "    Claims_map = mappings['Claim_Object_Map']\n",
    "    Account_map = mappings['Account_Object_Map']\n",
    "    Povider_map = mappings['Provider_Object_Map']\n",
    "    lines_map = mappings['Line_Item_Object_Map']\n",
    "\n",
    "    Errors = {}\n",
    "    for Source_doc_name in Source_data:\n",
    "        Source_doc = Source_data[Source_doc_name]\n",
    "        print(Source_doc_name)\n",
    "        print(Source_doc.shape)\n",
    "\n",
    "\n",
    "        ##upsert account and provider records\n",
    "        map_account_table, _ = create_df(Account_map, Source_doc, sf)\n",
    "        map_provider_table, keys = create_df(Povider_map, Source_doc, sf)\n",
    "        print('upserting to providers ... ')\n",
    "        new_ids, errors = upsert(map_provider_table.drop_duplicates(), keys, sf)\n",
    "        Errors[(Source_doc_name, 'providers_obj')] = errors ### need to replace source doc with actual string representation\n",
    "\n",
    "\n",
    "        ##### populate claims obj on sf\n",
    "        map_claims_table, keys = create_df(Claims_map, Source_doc, sf)\n",
    "        print('Inserting claims data ... ')\n",
    "        new_ids_claims, errors = insert_records(map_claims_table.drop_duplicates(), keys, sf) ###insert new claims records\n",
    "        Errors[(Source_doc_name, 'claims_obj')] = errors\n",
    "\n",
    "        ##### populate lines obj on sf\n",
    "        map_lines_table, keys = create_df(lines_map, Source_doc, sf, new_ids_claims)\n",
    "        print('Inserting lines data ... ')\n",
    "        new_ids_lines, errors = insert_records(map_lines_table.drop_duplicates(), keys, sf) ###insert new line records\n",
    "        Errors[(Source_doc_name, 'lines_obj')] = errors\n",
    "\n",
    "    return Errors\n",
    "\n",
    "Errors = ingest_data(json_path, Source_data, sf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in Errors:\n",
    "    print(key)\n",
    "\n",
    "len(Errors[('837I00111182024.csv', 'claims_obj')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "claims_sf = updated_data_pull(['Claims__c'], sf)\n",
    "Line_items_sf = updated_data_pull(['Line_Items__c'], sf)\n",
    "print(claims_sf.shape) ## original shape is 12,937\n",
    "print(Line_items_sf.shape) ## original shape is 22,841\n",
    "\n",
    "# 13040, 23266 after first file ingested\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### delete records\n",
    "\n",
    "Source_data_df = Source_data['837I00111182024.csv']\n",
    "print(Source_data_df.shape)\n",
    "# Source_data_df = Source_data['837P00111182024.csv']\n",
    "# print(Source_data_df.shape)\n",
    "\n",
    "source_cases = Source_data_df.drop_duplicates(subset=['Claim ID'])[['Claim ID']]\n",
    "ids = list(source_cases.merge(updated_data_pull(['Claims__c'], sf), left_on='Claim ID', right_on='Claim_ID__c').loc[:, ['Claim ID', 'Claim_ID__c', 'Id']]['Id'])\n",
    "keys = {'Claims__c':'Claim_ID__c'}\n",
    "delete_record(ids, keys, sf)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
