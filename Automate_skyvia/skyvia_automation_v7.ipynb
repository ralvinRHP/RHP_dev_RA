{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection established successfully!\n",
      "Files in directory: ['837I00111182024.csv', '837P00111182024.csv']\n",
      "File 837I00111182024.csv read into DataFrame successfully\n",
      "File 837P00111182024.csv read into DataFrame successfully\n",
      "Connection closed.\n",
      "837I00111182024.csv\n",
      "Source data shape: (446, 42)\n",
      "837P00111182024.csv\n",
      "Source data shape: (3106, 42)\n",
      "Connected to Salesforce sandbox\n",
      "Connected to Salesforce Prod\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('/mnt/c/Users/ralvin/OneDrive - Reliant Health Partners/Documents/RHP_dev_RA/Automate_skyvia')\n",
    "import pandas as pd\n",
    "from simple_salesforce import Salesforce, SalesforceLogin\n",
    "import pyodbc, sys, time \n",
    "import requests\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from helper_functions_v4 import create_df, updated_data_pull, upsert, delete_record, insert_records, salesforce_connection, read_sftp_data, read_sftp_directory, data_pull\n",
    "import configparser\n",
    "import json\n",
    "import paramiko\n",
    "import io\n",
    "\n",
    "pd.set_option('display.max_rows', None) \n",
    "pd.set_option('display.max_columns', None) \n",
    "\n",
    "\n",
    "##### retrieve source data\n",
    "config = configparser.ConfigParser()\n",
    "config.read('config_v2.ini')\n",
    "sftp_config = config['sftp']\n",
    "mapping_config = config['mappings']\n",
    "host = sftp_config.get('host')\n",
    "port = sftp_config.getint('port', fallback=22) \n",
    "username = sftp_config.get('user')\n",
    "password = sftp_config.get('password')\n",
    "remote_dir = sftp_config.get('remote_path')\n",
    "json_path = mapping_config.get('json_file_path')\n",
    "\n",
    "\n",
    "\n",
    "Source_data = read_sftp_directory(remote_dir, host, port, username, password) ### list of all source dataframes\n",
    "for i in Source_data:\n",
    "    print(i)\n",
    "    print(f'Source data shape: {Source_data[i].shape}')\n",
    "\n",
    "## sf connection\n",
    "sf = salesforce_connection(sandbox=True)\n",
    "sf_prod = salesforce_connection(sandbox=False)\n",
    "\n",
    "\n",
    "# 13040, 23266 after first file ingested\n",
    "# 14167, 25784 after second file ingested\n",
    "# ~35 min to ingest both files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "837I00111182024.csv\n",
      "(446, 42)\n",
      "\n",
      "\n",
      "\n",
      "upserting to providers ... \n",
      "Total records inserted: 62\n",
      "Total errors logged: 0\n",
      "\n",
      "________________________________________________________________________________________\n",
      "pulling updated Provider_TIN__c table\n",
      "pulled Provider_TIN__c\n",
      "Merged TIN\n",
      "pulling updated Groups_Clients__c table\n",
      "pulled Groups_Clients__c\n",
      "Merged Patient Group/Policy Number\n",
      "pulling updated Jurisdiction__c table\n",
      "pulled Jurisdiction__c\n",
      "Merged JurisdictionState\n",
      "pulling updated DRG__c table\n",
      "pulled DRG__c\n",
      "Merged DRG\n",
      "pulling updated Provider_Specialty__c table\n",
      "pulled Provider_Specialty__c\n",
      "Merged Billing Provider Taxonomy\n",
      "Inserting claims data ... \n",
      "Total records inserted: 103\n",
      "Total errors logged: 6\n",
      "\n",
      "________________________________________________________________________________________\n",
      "pulling updated Claims__c table\n",
      "pulled data slice!\n",
      "Merged Claim ID\n",
      "pulling updated HCPCS_CPT_Code__c table\n",
      "pulled HCPCS_CPT_Code__c\n",
      "Merged HCPCS/CPT Code\n",
      "Inserting lines data ... \n",
      "Total records inserted: 425\n",
      "Total errors logged: 21\n",
      "\n",
      "________________________________________________________________________________________\n",
      "837P00111182024.csv\n",
      "(3106, 42)\n",
      "\n",
      "\n",
      "\n",
      "upserting to providers ... \n",
      "Total records inserted: 698\n",
      "Total errors logged: 0\n",
      "\n",
      "________________________________________________________________________________________\n",
      "pulling updated Provider_TIN__c table\n",
      "pulled Provider_TIN__c\n",
      "Merged TIN\n",
      "pulling updated Groups_Clients__c table\n",
      "pulled Groups_Clients__c\n",
      "Merged Patient Group/Policy Number\n",
      "pulling updated Jurisdiction__c table\n",
      "pulled Jurisdiction__c\n",
      "Merged JurisdictionState\n",
      "pulling updated DRG__c table\n",
      "pulled DRG__c\n",
      "Merged DRG\n",
      "pulling updated Provider_Specialty__c table\n",
      "pulled Provider_Specialty__c\n",
      "Merged Billing Provider Taxonomy\n",
      "Inserting claims data ... \n",
      "Total records inserted: 1230\n",
      "Total errors logged: 74\n",
      "\n",
      "________________________________________________________________________________________\n",
      "pulling updated Claims__c table\n",
      "pulled Claims__c\n",
      "Merged Claim ID\n",
      "pulling updated HCPCS_CPT_Code__c table\n",
      "pulled HCPCS_CPT_Code__c\n",
      "Merged HCPCS/CPT Code\n",
      "Inserting lines data ... \n",
      "Total records inserted: 2943\n",
      "Total errors logged: 163\n",
      "\n",
      "________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def ingest_data(json_path, Source_data, sf):\n",
    "    ## get data mappings \n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    mappings = {k:v for k,v in data.items()}\n",
    "    Claims_map = mappings['Claim_Object_Map']\n",
    "    Account_map = mappings['Account_Object_Map']\n",
    "    Povider_map = mappings['Provider_Object_Map']\n",
    "    lines_map = mappings['Line_Item_Object_Map']\n",
    "\n",
    "    Errors = {}\n",
    "    for Source_doc_name in list(Source_data.keys())[:]:\n",
    "        Source_doc = Source_data[Source_doc_name]\n",
    "        print(Source_doc_name)\n",
    "        print(Source_doc.shape)\n",
    "        print('')\n",
    "        print('')\n",
    "        print('')\n",
    "\n",
    "\n",
    "        ##upsert account and provider records\n",
    "        map_account_table, _ = create_df(Account_map, Source_doc, sf)\n",
    "        map_provider_table, keys = create_df(Povider_map, Source_doc, sf)\n",
    "        print('upserting to providers ... ')\n",
    "        new_ids, errors = upsert(map_provider_table.drop_duplicates(), keys, sf)\n",
    "        Errors[(Source_doc_name, 'providers_obj')] = errors ### need to replace source doc with actual string representation\n",
    "        print('')\n",
    "        print('________________________________________________________________________________________')\n",
    "\n",
    "\n",
    "        ##### populate claims obj on sf\n",
    "        map_claims_table, keys = create_df(Claims_map, Source_doc, sf)\n",
    "        print('Inserting claims data ... ')\n",
    "        new_ids_claims, errors = insert_records(map_claims_table.drop_duplicates(), keys, sf) ###insert new claims records\n",
    "        Errors[(Source_doc_name, 'claims_obj')] = errors\n",
    "        print('')\n",
    "        print('________________________________________________________________________________________')\n",
    "\n",
    "        ##### populate lines obj on sf\n",
    "        map_lines_table, keys = create_df(lines_map, Source_doc, sf, new_ids_claims)\n",
    "        print('Inserting lines data ... ')\n",
    "        new_ids_lines, errors = insert_records(map_lines_table.drop_duplicates(), keys, sf) ###insert new line records\n",
    "        Errors[(Source_doc_name, 'lines_obj')] = errors\n",
    "        print('')\n",
    "        print('________________________________________________________________________________________')\n",
    "\n",
    "    return Errors\n",
    "\n",
    "Errors = ingest_data(json_path, Source_data, sf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('837I00111182024.csv', 'providers_obj'),\n",
       " ('837I00111182024.csv', 'claims_obj'),\n",
       " ('837I00111182024.csv', 'lines_obj'),\n",
       " ('837P00111182024.csv', 'providers_obj'),\n",
       " ('837P00111182024.csv', 'claims_obj'),\n",
       " ('837P00111182024.csv', 'lines_obj')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(Errors.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pulled Claims__c\n",
      "pulled Line_Items__c\n",
      "(14270, 101)\n",
      "(26209, 49)\n"
     ]
    }
   ],
   "source": [
    "claims_sf = updated_data_pull(['Claims__c'], sf)\n",
    "Line_items_sf = updated_data_pull(['Line_Items__c'], sf)\n",
    "print(claims_sf.shape) ## original shape is 12,937\n",
    "print(Line_items_sf.shape) ## original shape is 22,841\n",
    "\n",
    "# 13040, 23266 after first file ingested\n",
    "# 14167, 25784 after second file ingested\n",
    "# 14270, 26209 after both files ingested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "837I00111182024.csv\n",
      "pulled Claims__c\n",
      "103\n",
      "Object: Claims__c, Records to process: 103\n",
      "All records deleted successfully.\n",
      "837P00111182024.csv\n",
      "pulled Claims__c\n",
      "1230\n",
      "Object: Claims__c, Records to process: 1230\n",
      "All records deleted successfully.\n"
     ]
    }
   ],
   "source": [
    "### delete all cases \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_source_ids(Source_data_df):\n",
    "    source_cases = Source_data_df.drop_duplicates(subset=['Claim ID'])[['Claim ID']]\n",
    "    ids = list(source_cases.merge(updated_data_pull(['Claims__c'], sf), left_on='Claim ID', right_on='Claim_ID__c').loc[:, ['Claim ID', 'Claim_ID__c', 'Id']]['Id'])\n",
    "    keys = {'Claims__c':'Claim_ID__c'}\n",
    "    print(len(ids))\n",
    "    delete_record(ids, keys, sf)\n",
    "\n",
    "\n",
    "#### delete all claims from all files ingested\n",
    "for key in Source_data:\n",
    "    print(key)\n",
    "    Source_data_df = Source_data[key]\n",
    "    get_source_ids(Source_data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
